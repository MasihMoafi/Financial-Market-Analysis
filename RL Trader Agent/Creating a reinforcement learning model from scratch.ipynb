{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cdf614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load\n",
    "data = pd.read_csv(r'/home/masih/Desktop/btc-usd-max.csv', parse_dates=['date'], index_col='date')\n",
    "\n",
    "# Split\n",
    "split_idx = int(len(data) * 0.8)\n",
    "train_data = data.iloc[:split_idx]\n",
    "test_data = data.iloc[split_idx:]\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_data[['price', 'total_volume']])\n",
    "test_scaled = scaler.transform(test_data[['price', 'total_volume']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "691ef1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Env Design\n",
    "\n",
    "class TradingEnv:\n",
    "    def __init__(self, data, window_size):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.current_step = window_size\n",
    "        self.max_step = len(data) - 1\n",
    "\n",
    "        # Init portfo\n",
    "        self.cash = 10000  # Starting cash\n",
    "        self.btc = 0.0\n",
    "        self.total_value = self.cash\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.cash = 10000\n",
    "        self.btc = 0.0\n",
    "        self.total_value = self.cash\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Get the past 'window_size' days' data\n",
    "        state = self.data[self.current_step - self.window_size : self.current_step]\n",
    "        return state.flatten()  # Flatten to 1D array for the neural net\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1, 2], \"Invalid action\"\n",
    "\n",
    "        # Get current price (scaled) and actual price (unscaled)\n",
    "        scaled_price = self.data[self.current_step, 0]\n",
    "        actual_price = scaler.inverse_transform(self.data[self.current_step].reshape(1, -1))[0, 0]\n",
    "\n",
    "        prev_value = self.total_value\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Execute action\n",
    "        if action == 0:  # Buy\n",
    "            if self.cash > 0:\n",
    "                # Use 10% of cash to buy BTC\n",
    "                investment = 0.1 * self.cash\n",
    "                self.btc += investment / actual_price\n",
    "                self.cash -= investment\n",
    "        elif action == 1:  # Sell\n",
    "            if self.btc > 0:\n",
    "                # Sell 10% of BTC\n",
    "                sell_amount = 0.1 * self.btc\n",
    "                self.cash += sell_amount * actual_price\n",
    "                self.btc -= sell_amount\n",
    "\n",
    "        # Update total portfolio value\n",
    "        self.total_value = self.cash + self.btc * actual_price\n",
    "\n",
    "        # Calculate reward as the change in portfolio value\n",
    "        reward = self.total_value - prev_value\n",
    "\n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        if self.current_step > self.max_step:\n",
    "            done = True\n",
    "\n",
    "        next_state = self._get_state() if not done else None\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d3eb384-d8d0-4e33-a4ec-f116917585a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5 completed. Epsilon: 0.9950\n",
      "Episode 2/5 completed. Epsilon: 0.9900\n",
      "Episode 3/5 completed. Epsilon: 0.9851\n",
      "Episode 4/5 completed. Epsilon: 0.9801\n",
      "Episode 5/5 completed. Epsilon: 0.9752\n"
     ]
    }
   ],
   "source": [
    "# dqn_agent_pt.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define a simple fully connected Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device='cpu'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95           # Discount factor\n",
    "        self.epsilon = 1.0          # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.device = device\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "        return int(torch.argmax(q_values[0]).item())\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, targets = [], []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_tensor = torch.FloatTensor(state).to(self.device)\n",
    "            # Get current Q-values\n",
    "            with torch.no_grad():\n",
    "                target = self.model(state_tensor.unsqueeze(0)).cpu().numpy()[0]\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                next_state_tensor = torch.FloatTensor(next_state).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    next_q = self.model(next_state_tensor.unsqueeze(0))\n",
    "                target[action] = reward + self.gamma * torch.max(next_q).item()\n",
    "            states.append(state)\n",
    "            targets.append(target)\n",
    "        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        targets_tensor = torch.FloatTensor(np.array(targets)).to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(states_tensor)\n",
    "        loss = self.criterion(outputs, targets_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Example usage (stub environment loop)\n",
    "if __name__ == \"__main__\":\n",
    "    state_size = 4   # Example state dimension (e.g., for CartPole)\n",
    "    action_size = 2  # Example number of actions\n",
    "    agent = DQNAgent(state_size, action_size, device='cpu')\n",
    "    episodes = 5\n",
    "    batch_size = 32\n",
    "\n",
    "    # Dummy environment loop for demonstration\n",
    "    for e in range(episodes):\n",
    "        state = np.random.rand(state_size)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done and step < 50:\n",
    "            action = agent.act(state)\n",
    "            next_state = np.random.rand(state_size)\n",
    "            reward = 1.0\n",
    "            done = (step == 49)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        agent.replay(batch_size)\n",
    "        print(f\"Episode {e+1}/{episodes} completed. Epsilon: {agent.epsilon:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c7bff40-6cb0-4476-a6a4-2d4f59eec39f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/50, Total Reward: $651315.50, Epsilon: 0.01\n",
      "Episode: 2/50, Total Reward: $41882.73, Epsilon: 0.01\n",
      "Episode: 3/50, Total Reward: $72409.25, Epsilon: 0.01\n",
      "Episode: 4/50, Total Reward: $440353.17, Epsilon: 0.01\n",
      "Episode: 5/50, Total Reward: $792.54, Epsilon: 0.01\n",
      "Episode: 6/50, Total Reward: $208669.01, Epsilon: 0.01\n",
      "Episode: 7/50, Total Reward: $70120.39, Epsilon: 0.01\n",
      "Episode: 8/50, Total Reward: $167700.22, Epsilon: 0.01\n",
      "Episode: 9/50, Total Reward: $292444.95, Epsilon: 0.01\n",
      "Episode: 10/50, Total Reward: $161570.85, Epsilon: 0.01\n",
      "Episode: 11/50, Total Reward: $322406.28, Epsilon: 0.01\n",
      "Episode: 12/50, Total Reward: $337588.21, Epsilon: 0.01\n",
      "Episode: 13/50, Total Reward: $180410.69, Epsilon: 0.01\n",
      "Episode: 14/50, Total Reward: $423283.73, Epsilon: 0.01\n",
      "Episode: 15/50, Total Reward: $213636.80, Epsilon: 0.01\n",
      "Episode: 16/50, Total Reward: $3725351.29, Epsilon: 0.01\n",
      "Episode: 17/50, Total Reward: $201238.96, Epsilon: 0.01\n",
      "Episode: 18/50, Total Reward: $4312623.91, Epsilon: 0.01\n",
      "Episode: 19/50, Total Reward: $172221.93, Epsilon: 0.01\n",
      "Episode: 20/50, Total Reward: $151393.54, Epsilon: 0.01\n",
      "Episode: 21/50, Total Reward: $58080.79, Epsilon: 0.01\n",
      "Episode: 22/50, Total Reward: $107451.56, Epsilon: 0.01\n",
      "Episode: 23/50, Total Reward: $213935.94, Epsilon: 0.01\n",
      "Episode: 24/50, Total Reward: $129906.30, Epsilon: 0.01\n",
      "Episode: 25/50, Total Reward: $1717967.98, Epsilon: 0.01\n",
      "Episode: 26/50, Total Reward: $1551293.00, Epsilon: 0.01\n",
      "Episode: 27/50, Total Reward: $1720803.80, Epsilon: 0.01\n",
      "Episode: 28/50, Total Reward: $1722121.06, Epsilon: 0.01\n",
      "Episode: 29/50, Total Reward: $1569670.13, Epsilon: 0.01\n",
      "Episode: 30/50, Total Reward: $2460510.10, Epsilon: 0.01\n",
      "Episode: 31/50, Total Reward: $2315427.28, Epsilon: 0.01\n",
      "Episode: 32/50, Total Reward: $1825959.90, Epsilon: 0.01\n",
      "Episode: 33/50, Total Reward: $1879134.69, Epsilon: 0.01\n",
      "Episode: 34/50, Total Reward: $2780675.85, Epsilon: 0.01\n",
      "Episode: 35/50, Total Reward: $3010849.98, Epsilon: 0.01\n",
      "Episode: 36/50, Total Reward: $2550122.07, Epsilon: 0.01\n",
      "Episode: 37/50, Total Reward: $2908550.01, Epsilon: 0.01\n",
      "Episode: 38/50, Total Reward: $2865155.99, Epsilon: 0.01\n",
      "Episode: 39/50, Total Reward: $2898287.29, Epsilon: 0.01\n",
      "Episode: 40/50, Total Reward: $2882659.05, Epsilon: 0.01\n",
      "Episode: 41/50, Total Reward: $2696205.14, Epsilon: 0.01\n",
      "Episode: 42/50, Total Reward: $2889054.72, Epsilon: 0.01\n",
      "Episode: 43/50, Total Reward: $3408791.80, Epsilon: 0.01\n",
      "Episode: 44/50, Total Reward: $2917560.16, Epsilon: 0.01\n",
      "Episode: 45/50, Total Reward: $3170863.76, Epsilon: 0.01\n",
      "Episode: 46/50, Total Reward: $2868379.00, Epsilon: 0.01\n",
      "Episode: 47/50, Total Reward: $2878466.26, Epsilon: 0.01\n",
      "Episode: 48/50, Total Reward: $3267848.94, Epsilon: 0.01\n",
      "Episode: 49/50, Total Reward: $2980193.55, Epsilon: 0.01\n",
      "Episode: 50/50, Total Reward: $3635420.04, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "WINDOW_SIZE = 10\n",
    "BATCH_SIZE = 32\n",
    "EPISODES = 50\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = TradingEnv(train_scaled, WINDOW_SIZE)\n",
    "state_size = WINDOW_SIZE * 2  # 2 features (price and volume)\n",
    "action_size = 3\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        # Train with replay\n",
    "        agent.replay(BATCH_SIZE)\n",
    "\n",
    "    print(f\"Episode: {episode+1}/{EPISODES}, Total Reward: ${total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acad300-a852-4fdf-9ad4-b8e2c61990b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TradingEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_env \u001b[38;5;241m=\u001b[39m TradingEnv(test_scaled, WINDOW_SIZE)\n\u001b[1;32m      2\u001b[0m state \u001b[38;5;241m=\u001b[39m test_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TradingEnv' is not defined"
     ]
    }
   ],
   "source": [
    "test_env = TradingEnv(test_scaled, WINDOW_SIZE)\n",
    "state = test_env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, reward, done = test_env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "print(f\"Test Total Reward: ${total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfcbfc-d9d7-4646-af7c-ec90feb3b835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
